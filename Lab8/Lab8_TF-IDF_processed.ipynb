{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ef2df5-f1a1-4283-b756-bd309bfffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1dc86e-3c42-4c89-8d35-a2c696c9df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    "\"I enjoy hiking and camping in the mountains\",\n",
    "\"I like to read books and watch movies\",\n",
    "\"I prefer playing video games over sports\",\n",
    "\"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f43b0a-afca-4bb3-93f2-41f81f1c59d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess a sentence\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # 1. Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 2. Remove punctuation\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # 3. Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply to dataset\n",
    "preprocessed_dataset = [preprocess(sentence) for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef956567-522e-4d97-bb39-064bda057ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbdc51b-7b98-4efa-be47-ec555e3a174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                               Predicted Cluster\n",
      "-----------------------------------  -------------------\n",
      "love playing football weekends                         0\n",
      "enjoy hiking camping mountains                         1\n",
      "like read books watch movies                           0\n",
      "prefer playing video games sports                      0\n",
      "love listening music going concerts                    0\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " love\n",
      "\n",
      " playing\n",
      "\n",
      " football\n",
      "\n",
      " weekends\n",
      "\n",
      " going\n",
      "\n",
      " sports\n",
      "\n",
      " music\n",
      "\n",
      " concerts\n",
      "\n",
      " video\n",
      "\n",
      " games\n",
      "\n",
      "Cluster 1:\n",
      " camping\n",
      "\n",
      " enjoy\n",
      "\n",
      " hiking\n",
      "\n",
      " mountains\n",
      "\n",
      " weekends\n",
      "\n",
      " listening\n",
      "\n",
      " concerts\n",
      "\n",
      " football\n",
      "\n",
      " games\n",
      "\n",
      " going\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(preprocessed_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43110fc2-b173-4268-b92d-a4bc331ec276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78311b-8819-4bd1-a3eb-5d48bab8b3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
